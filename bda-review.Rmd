# How to Get the Most Out of BDA3

Our thinking about Bayesian statistics is summarized in the book Bayesian Data Analysis, third edition (BDA3) by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin.  A similar perspective is presented more concisely in the book Statistical Rethinking: A Bayesian Course with Examples in R and Stan by McElreath.

There is no point in repeating the material in BDA3 here.  Instead we go through it, chapter by chapter, pointing out the parts that should be most helpful for learning Bayesian workflow using Stan.  Then we list some important topics that _should_ be in BDA3 but are not there.

## BDA3 part I:  Fundamentals of Bayesian inference {-}

Much of the material in this first part of the book, especially in chapters 2, 3, and 5, concerns the algebra that is, if not obsolete, at least not as important as it once was.  But these chapters also introduce some key principles in the context of simple but realistic examples.  Hence we attempt some guidance about what to read so as to enter the Bayesian way of thinking without getting lost in now-irrelevant details.

## BDA3 chapter 1:  Probability and inference {-}

You should read this chapter if for no other reason than to overwrite various misconceptions you have about probability and Bayesian inference.

The necessary sections in chapter 1 for you to read:

* Overview:  section 1.1.

* Spelling correction example:  part of section 1.4, just pages 9-11.  We work out the mechanics of Bayesian analysis for a discrete problem (three possibilities) and then discuss extensions of the model.  This follows our general template of model building, inference, and expansion.

* Foundations of probability, our pragmatic perspective:  section 1.5.

If you want to get more into Bayesian philosophy, we recommend these two articles:
Gelman and Shalizi, Philosophy and the practice of Bayesian statistics (http://www.stat.columbia.edu/~gelman/research/published/philosophy.pdf) or shorter version here: http://www.stat.columbia.edu/~gelman/research/published/philosophy_online4.pdf),
Gelman and Hennig, Beyond subjective and objective in statistics (http://www.stat.columbia.edu/~gelman/research/published/gelman_hennig_full_discussion.pdf)

## BDA3 chapters 2 and 3:  Single-parameter models and Introduction to multiparameter models {-}

Now that we have Stan we can fit complicated models all at once.  But back in the early 1990s when we were writing BDA, each parameter represented an effort, and we worked as much as possible with ``conjugate priors'':  models where the posterior distribution can be written analytically.

At this point, chapters 2 and 3 can be mostly thought of as reference materials, where we derive various standard analytic results.

The necessary sections in chapters 2 and 3 for you to read:

* Posterior as compromise between data and prior:  section 2.2.

* Prior-posterior calculations with the normal distribution:  section 2.5.  You don't really need to know this stuff, but it's good to know where the derivation is, so you can see how such analyses are performed.

* Partial pooling for estimating the rats of a rare cancer:  section 2.7.  This is one of our favorite examples.  The only thing is, don't obsess about how the posterior distribution is estimated from the data.  When this section was written, it simplest to fit this distribution using the method of moments, matching the mean and variance of the empirical distribution of cancer rates to their expectations under the model.  This introduces lots of complications, and it's much easier just to fit the model directly in Stan.  Set aside the details of fitting, though, and it's an excellent example.  One thing we like about it is that it demonstrates the role of the prior distribution and data, through a model with data from 3000 counties but a single prior distribution.

* Weakly informative priors:  section 2.9.  Our thinking has advanced since this section was written (see the Prior Choice Wiki), but this is still a good two-page introduction.

* Averaging over parameters:  section 3.1.  The math in this short section isn't so relevant, but the general idea expressed here could be useful when thinking about multidimensional distributions.

* Simple logistic regression:  section 3.7.  This is a good simple example going through detailed steps of model building, computation, and inferential summary--not including model checking and expansion because this particular dataset is so small that there's nothing much to check. The only thing that's missing is a graph of data and fitted model, but that's something you can do as an exercise.  In this example we use old-fashioned computation, but it's simple summing over a grid, no mathematical integrals to worry about.  We recommend that you go to the trouble of understanding this computation, even though in future you'll be fitting this sort of model in Stan.

## BDA3 chapter 4:  Asymptotics and connections to non-Bayesian approaches {-}

The asymptotic normality of the posterior distribution is something that used to be more important, back when a common method of summarizing a posterior was to compute its mean and curvature.  That said, these ideas are still important for several reasons:

* It can be helpful to understand what happens to Bayesian inferences as more data arrive--while also recognizing where asymptotic approximations fail.

* You will often encounter approximate or non-Bayesian computations--estimates, standard errors, hypothesis tests--and you'll want to know how to integrate these into Bayesian thinking.

* Nowadays it is rare that we would approximate a posterior distribution by posterior mode and uncertainty, but this approach can be useful as a building block for complicated problems, as we discuss in chapter 13 on modal and distributional approximations.

The necessary sections in chapter 4 for you to read:

* Large-sample theory:  section 4.2.  Read the whole section if you can easily follow the math; otherwise skip any tricky notation and just read the words.  Don't get us wrong here:  we _do_ believe this math is important.  It's just not something to worry about when getting started.  Later on you can go back and learn it as needed.  It's similar to how you don't need to have much understanding of physics to drive a car, but if you want to start tinkering with your car, or building a new one, you'll want to know more.

* Counterexamples to large-sample results:  section 4.3.  Read this whole section carefully, _including all the math_.  The examples here are important.

* Bayesian interpretations of other statistical methods:  section 4.5.  You'll need to know this too.

## BDA3 chapter 5:  Hierarchical models {-}

In simple Bayesian inference, there is a fixed prior representing specific information about a parameter (for example, theta ~ normal(4.3, 1.2)) and a fixed data model giving additional information about theta, and these are combined to yield posterior inference.  One of the stumbling blocks here is that we don't always feel comfortable specifying a precise numerical prior, but we do have relevant prior information

The necessary sections in chapter 5 for you to read:

* Estimating a prior distribution from data:  section 5.1.  This presents the basic idea.  In this example the hyperparameters are estimated using the method of moments, and we would not recommend that now--indeed, it's a bit of a distraction in the exposition--so we recommend you redo the example yourself using Stan.  Still read the whole section, though.

* Estimating treatment effects in eight schools:  section 5.5.  The discussion and graphs surrounding this classic example are still very much worth reading.

* Hierarchical modeling for a meta-analysis:  section 5.6.  This section is not necessary but you can read it if you want to see another worked example.

* Priors for hierarchical variance parameters:  section 5.7.  Our thinking has advanced somewhat since this section was written, but it's still a good starting point for thinking about hyperprior distributions in complex models.

## BDA3 part II:  Fundamentals of Bayesian data analysis {-}

These are the key chapters in BDA3 because they go beyond Bayesian _inference_ (learning through a particular model) to aspects of Bayesian _workflow_ (evaluating and using models).

## BDA3 chapter 6:  Model checking {-}

Before BDA was written, there was very little in the statistics literature on the checking of Bayesian models.  So this chapter provides both explication and justification of these methods.

The necessary sections in chapter 6 for you to read:

* Overview of Bayesian model checking:  section 6.1.

* Comparing fitted models to other knowledge:  section 6.2.

* Posterior predictive checking:  part of section 6.3, just pages 143-144, but then you can skip the rest of the section which is mostly about p-values, which we don't recommend any more.

* Graphical posterior predictive checks:  section 6.4.  Several examples here, each illustrating different principles.

* Model checking for eight schools example:  section 6.5.  This is valuable not so much for the graphs and p-values, which don't show much, but for the discussion of the modeling assumptions and how they can be checked from data.

## BDA3 chapter 7:  Evaluating, comparing, and expanding models {-}

This chapter is fairly theoretical, as it represented our best effort to understand predictive-based model evaluation.  The key idea is that when evaluating or comparing models in this way, we are not interested in the posterior probability of the model but rather in how well it predicts.  Thus, a model can perform well for some purposes but not for others.

For practical purposes, we recommend this article published in 2017 that gives theory, Stan implementation, and several examples of predictive model evaluation and comparison: http://www.stat.columbia.edu/~gelman/research/published/loo_stan.pdf.  That said, chapter 7 of BDA3 may be useful in that it presents the key ideas in compact form.

The necessary sections in chapter 7 for you to read:

* Measures of predictive accuracy:  section 7.1.  This has some algebra, but it's algebra that any serious Bayesian modeler or Stan user should know, involving the logarithm of the posterior density function (the target or objective function in a Stan model).

* Information criteria:  section 7.2.  You'll need to read this section if you want to understand the connections between cross validation, AIC, BIC, and other information scores that adjust for effective number of parameters in a fitted model. Feel free to skip the details here unless you really care about all these different methods, but there are lots of interesting theoretical ideas here.

* Bayes factors and continuous model expansion:  sections 7.4 and 7.5.  We don't recommend Bayes factors for model comparison; instead we prefer continuous model expansion.  These sections explain why.

* Example of implicit assumptions and model failure:  section 7.6.  You can skip this self-contained case study, but we recommend you read it if you have the time, as it's a simple example of how a reasonable-seeming model can go wrong, how this problem can be found using predictive checking.  This example is also valuable because it demonstrates that a statistical procedure applied to a particular class of problems can include implicit assumptions--and when these assumptions are made explicit, they can improve the model.

## BDA3 chapter 8:  Modeling accounting for data collection {-}

In this theoretically-oriented chapter, we work out some of the implications of Bayesian inference in sampling, causal inference, and selection:  all contexts where the model is fit to data that are not necessarily representative of the population that is the target of study.  The key idea is to include in the Bayesian model an ``inclusion'' variable with a probability distribution that represents the process by which data become observed.

The necessary sections in chapter 8 for you to read:

* General framework for Bayesian modeling accounting for for data collection:  section 8.1.

* Randomization in Bayesian inference:  section 8.5.

* Observational studies: part of section 8.6, just pages 220-222.

* Censoring and truncation:  section 8.7.  You can skip this section for now, but it will be useful if you want to work with data that have selection issues.

* Summary of when you can ignore details of the data collection process:  section 8.8.

The other parts of chapter 8 have interesting material, but you don't need them to get started.

## BDA3 chapter 9:  Decision analysis {-}

This chapter has examples of Bayesian decision analyses in medicine, social science, and public health, in each case assessing expected costs and benefits by averaging over a posterior distribution.

The necessary sections in chapter 9 for you to read:

* Bayesian decision theory:  section 9.1.

* Personal vs. institutional decision analysis:  section 9.5.

The other sections are fine too; they're just pretty detailed examples.  It would be good to have a simpler but still real example to demonstrate decision analysis within Bayesian workflow.

## BDA3 part III:  Advanced computation {-}

But now that you can fit models in Stan, why do you need to learn the mathematics of Bayesian computing?  You can drive a car without understanding the workings of the internal combustion engine; why can't you fit models in Stan without knowing how Hamiltonian Monte Carlo works?

The short answer is that, yes, you _can_ fit models in Stan without following what is going on under the hood, but there are good reasons for gaining a deeper understanding.  When a model is difficult to fit, you might be able to reparameterize it or change it in some way so it will fit better.

## BDA3 chapter 10:  Introduction to Bayesian computation {-}

Chapter 10 covers the basic principles of simulation-based Bayesian inference, focusing on simple problems where it is possible to sample directly from the posterior distribution.

The necessary sections in chapter 10 for you to read:

* Rejection sampling:  section 10.3.  You may never use this in a real problem, but this simple method illustrates some general principles, so it's worth reading this section carefully and programming up the example yourself in R or Python.

* How many simulation draws are needed:  section 10.5.

* Debugging Bayesian computing:  section 10.7.  This short section is worth reading, even though the present book updates much of this advice.

## BDA3 chapter 11:  Basics of Markov chain simulation {-}

Until recently, the Gibbs sampler and Metropolis algorithm were the standard approaches for general Bayesian simulation.  We have now mostly moved on, but it can be helpful to learn these algorithms as a first step toward more advanced approaches.

The necessary sections in chapter 11 for you to read:

* Markov chain simulation:  before section 11.1, pages 275-276.

* Inference and assessing convergence:  part of section 11.4, pages 281-284.  You don't need to follow all the details here, but you should understand the general principles of monitoring the mixing of simulations, as illustrated in Figures 11.1 and 11.3. For more on assessing convergence and effective sample size, see this article by Aki Vehtari et al.:  http://www.stat.columbia.edu/~gelman/research/unpublished/1903.08008.pdf

## BDA3 chapter 12:  Computationally efficient Markov chain simulation {-}

When getting started, you don't need to read any of this chapter.  If you want to understand Hamiltonian Monte Carlo and challenges in Bayesian computation more generally, you should read this whole chapter and also work through the R code in appendix C, where we perform posterior inference for the eight schools model using several different simulation algorithms.  We also recommend this article by Radford Neal:  MCMC using Hamiltonian dynamics:  https://arxiv.org/pdf/1206.1901.pdf

## BDA3 chapter 13:  Modal and distributional approximations {-}

Some models are just too difficult or too big to fit using fully Bayesian inference and hence you need to use some set of these approximation strategies:

* Fitting a model to some summary or subsets of the data

* Fitting a simpler model to your data

* Giving up on full posterior simulation; summarizing the posterior in some way.

In practice we use all these approaches.  We restrict the range of application of our analysis rather than trying to incorporate all possible data; we work with models that necessarily simplify the world; and if necessary we approximate our computations.

Chapter 13 discusses some particular strategies for approximate computation.  As with the previous chapter, you don't need to read any of this to get started, but this material can be a useful resource when you enounter problems for which approximations are necessary.

When you get to that point, we recommend you read sections 13.7 (Variational inference), 13.8 (Expectation propagation), and 13.10 (Unknown normalizing factors), which give Bayesian-focused explanations that are complementary to standard descriptions of these methods that appear elsewhere in the literature.

## BDA3 parts IV and V:  Regression models and Nonlinear and nonparametric models {-}

The remaining chapters in BDA3 cover specific classes of models, so you can dip into them as needed.  Below we point to some particular sections that can be worth reading right away.

## BDA3 chapter 14:  Introduction to regression models {-}

The necessary sections in chapter 14 for you to read:

* Conditional modeling:  section 14.1.  Important for explaining what it means to condition on predictors in a Bayesian framework.  The ideas here are also relevant to Stan, in allowing you to understand the distinction between unmodeled data, modeled data, and parameters, as discussed in the chapter on Simulating Data in this Bayesian Workflow book.

* Bayesian analysis of classical regression:  section 14.2.  You might not need this right away, but if you're ever going to get into computational efficiency and approximate algorithms, the matrix algebra here will be relevant.

* Regression for causal inference: incumbency and voting:  section 14.3.  This section is essentially a case study.  Read it if you are planning to use Bayesian methods for causal inference in social science.

* Goals of regression analysis and Assembling the matrix of explanatory variables:  sections 14.4-14.5.  These sections offer general advice on building regression models.  For (much) more on the topic, we recommend the book Regression and Other Stories.

* Regularization and dimension reduction:  section 14.6.  This is more specifically Bayesian.  This section offers good concepts but not much on specific approaches; for that we currently point to the articles, Sparsity information and regularization in the horseshoe and other shrinkage priors (https://arxiv.org/abs/1707.01694) by Juho Piironen and Aki Vehtari, and Bayes sparse regression (https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html) by Michael Betancourt.

## BDA3 chapters 15 and 16:  Hierarchical linear models and Generalized linear models {-}

Hierarchical (multilevel) regression is one of the most important applications of Bayesian methods, for several reasons:

* Hierarchical models can have many parameters relative to data--to put it another way, data can be sparse--hence the relevance of prior information and partial pooling.

* Quantitities of interest typically involve many parameters (as with multilevel regression and poststratification) and predictions for new groups, and simulation-based Bayesian inference excels at propagating uncertainty in high-dimensional settings.

* Multilevel modeling provides a general template for combining data from different sources (for example, at the individual and state levels in a national survey, or from different experiments in a meta-analysis).

Unfortunately, it's hard for us to recommend any particular sections in these two chapters of BDA, as the computational methods described there are out of date, and for applied insight we recommend the book, Data Analysis Using Regression and Multilevel/Hierarchical Models, by Gelman and Hill.

## BDA3 chapter 17:  Models for robust inference {-}

An inference is _robust_ to the extent that it does not change much when various inputs are changed.  Robustness is not always a desirable property:  sometimes we want inferences to be sensitive to certain aspects of the data or model.  So we should think of robustness as a _description_ rather than a _virtue_ of a statistical method.

The necessary sections in chapter 17 for you to read:

* Aspects of robustness:  section 17.1.

* Overdispersed versions of standard models:  section 17.2.

The remaining sections of this chapter use obsolete computational methods.

## BDA3 chapter 18:  Models for missing data {-}

Missing data are unavoidable in real statistics problems.  Bayesian methods are appropriate in allowing us to account for uncertainty about missing observations.

The necessary sections in chapter 18 for you to read:

* Notation:  section 18.1.  The material here extends the approach of chapter 8, in which the statistical model needs to include all information that predicts data inclusion.

In practice, we often use various approximate approaches to missingness, imputing missing values using some approximate methods.

## BDA3 chapter 19:  Parametric nonlinear models {-}

This chapter gives two case studies of nonlinear models, one for the analysis of laboratory assays and one in toxicology.  We recommend both these examples, but because they don't yet have corresponding Stan code, they might not be the best examples for you to read right away.  Instead it makes more sense for you to start with the Stan case studies:  https://mc-stan.org/users/documentation/case-studies and the examples from StanCon:  https://github.com/stan-dev/stancon_talks.

## BDA3 chapter 20:  Basis function models {-}

Splines are a useful class of nonlinear regression models.  

The necessary sections in chapter 20 for you to read:

* Introduction to splines:  section 20.1.  This section defines a class of spline models and illustrates them with simulation from the prior and a fit to a small dataset.  Along with this you can read the case study by Milad Kharratzadeh:  https://mc-stan.org/users/documentation/case-studies/splines_in_stan.html.  A key step in Bayesian splines is the use of priors that enforce smoothness.  This sort of smoothing gives us more flexibility in modeling, as compared to cruder unsmoothed approaches in which smoothing is done by restricting the number of parameters in the model.

## BDA3 chapter 21:  Gaussian process models {-}

Gaussian processes are a general framework for models of dependence and are often used in spatial statistics or for prior distributions that enforce local smoothness.

The necessary sections in chapter 21 for you to read:

* Introduction to Gaussian processes:  section 21.1.

* Birthdays examples:  section 21.2.  Depending on your interests, you can skip many of the details of this model and focus on the fit (figure 21.4) and model improvement (figure 21.5 and related discussion).  This example is still open in the sense that the final model in section 21.2 still has problems which have not been resolved; see https://statmodeling.stat.columbia.edu/2016/05/18/birthday-analysis-friday-the-13th-update/.

In addition, we recommend two case studies on Gaussian processes in Stan, one by Lu Zhang at https://mc-stan.org/users/documentation/case-studies/nngp.html and one by Michael Betancourt at https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html.

## BDA3 chapter 22:  Finite mixture models {-}

The necessary sections in chapter 22 for you to read:

* Overview of mixture models:  section 22.1.

Also we recommmend the section on mixture models in the Stan User's Guide and this case study by Michael Betancourt:  https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html.

## BDA3 chapter 23:  Dirichlet process models {-}

You can read this entire chapter if you find yourself working with this class of model.

## BDA3 appendix A:  Standard probability distributions {-}

This appendix is a useful compendium of information on probability distributions, better than wikipedia because it uses consistent notation.

## BDA3 appendix B:  Outline of proofs of limit theorems {-}

This short section complements chapter 4 on the asymptotic limits of Bayesian inference.

## BDA3 appendix C:  Computation in R and Stan {-}

Much of this appendix involves programming directly in R the computations that are done automatically in Stan. But we recommend that you through these computations as a way of understanding basic implementations of the Gibbs sampler, Metropolis algorithm, and Hamiltonian Monte Carlo.

